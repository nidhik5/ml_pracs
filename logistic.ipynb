{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102f88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e4d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Iris dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Nidhi\\Desktop\\sem 7\\ml\\ML datasets\\ML datasets\\Iris\\Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b18bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode the species column to numerical values (for simplicity let's assume binary classification, Iris-setosa = 0)\n",
    "df['Species'] = df['Species'].apply(lambda x: 0 if x == 'Iris-setosa' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ffb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "y = df['Species'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Normalize the data (since logistic regression converges faster with normalized data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37db8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the data into training and test sets (since the dataset is small, we'll use all data for training)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ace70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function inline\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2fd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize parameters for logistic regression\n",
    "m_train, n_features = X_train.shape\n",
    "theta = np.zeros(n_features)  # weights for each feature\n",
    "bias = 0                      # bias term\n",
    "learning_rate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db71f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Training logistic regression using gradient descent\n",
    "for _ in range(iterations):\n",
    "    # Compute linear combination of inputs and weights\n",
    "    linear_model = np.dot(X_train, theta) + bias\n",
    "    \n",
    "    # Apply sigmoid function to get predicted probabilities\n",
    "    y_predicted = 1 / (1 + np.exp(-linear_model))  # sigmoid\n",
    "    \n",
    "    # Compute gradients for weights and bias\n",
    "    d_theta = (1 / m_train) * np.dot(X_train.T, (y_predicted - y_train))  # gradient of the cost w.r.t. theta\n",
    "    d_bias = (1 / m_train) * np.sum(y_predicted - y_train)                # gradient of the cost w.r.t. bias\n",
    "    \n",
    "    # Update weights and bias using gradient descent\n",
    "    theta -= learning_rate * d_theta\n",
    "    bias -= learning_rate * d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Making predictions on the training set\n",
    "linear_model_train = np.dot(X_train, theta) + bias\n",
    "y_pred_train_probs = 1 / (1 + np.exp(-linear_model_train))  # apply sigmoid\n",
    "y_pred_train = [1 if prob > 0.5 else 0 for prob in y_pred_train_probs]  # convert probabilities to binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34177345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a very small dataset, we'll use the same dataset for predictions\n",
    "linear_model_test = np.dot(X_test, theta) + bias\n",
    "y_pred_test_probs = 1 / (1 + np.exp(-linear_model_test))  # apply sigmoid\n",
    "y_pred_test = [1 if prob > 0.5 else 0 for prob in y_pred_test_probs]  # convert probabilities to binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b744c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Calculate accuracy\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0e7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Output the accuracy\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08799eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777e420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
